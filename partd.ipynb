{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "data_dir = os.getcwd()+\"/log_data/\"\n",
    "sys.argv = [\"file\",data_dir+\"train.csv\", data_dir+\"test.csv\", data_dir+\"param_a.txt\" ,data_dir+\"outputfile.csv\", data_dir+\"weightfile.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotencode_train(data):\n",
    "    train = pd.DataFrame()\n",
    "    parents = pd.get_dummies(data[0], prefix=\"parents\")\n",
    "    has_nurs = pd.get_dummies(data[1], prefix=\"has_nurs\")\n",
    "    form = pd.get_dummies(data[2], prefix=\"form\")\n",
    "    children = pd.get_dummies(data[3], prefix=\"children\")\n",
    "    housing = pd.get_dummies(data[4], prefix=\"housing\")\n",
    "    finance = pd.get_dummies(data[5], prefix=\"finance\")\n",
    "    social = pd.get_dummies(data[6], prefix=\"social\")\n",
    "    health = pd.get_dummies(data[7], prefix=\"health\")\n",
    "    classDistribution = pd.get_dummies(data[8], prefix=\"class\")\n",
    "    train = pd.concat([parents, has_nurs, form, children, housing,\n",
    "                    finance, social, health, classDistribution], axis=1)\n",
    "    cols = ['parents_usual', 'parents_pretentious', 'parents_great_pret', 'has_nurs_proper', 'has_nurs_less_proper', 'has_nurs_improper',\n",
    "            'has_nurs_critical', 'has_nurs_very_crit',\n",
    "            'form_complete', 'form_completed', 'form_incomplete', 'form_foster',\n",
    "            'children_1', 'children_2', 'children_3', 'children_more',\n",
    "            'housing_convenient', 'housing_less_conv', 'housing_critical',\n",
    "            'finance_convenient', 'finance_inconv',\n",
    "            'social_nonprob', 'social_slightly_prob', 'social_problematic',\n",
    "            'health_recommended', 'health_priority', 'health_not_recom',\n",
    "            'class_not_recom', 'class_recommend', 'class_very_recom', 'class_priority', 'class_spec_prior']\n",
    "\n",
    "    return train[pd.Index(cols)]\n",
    "\n",
    "def hotencode_test(data):\n",
    "    train = pd.DataFrame()\n",
    "    parents = pd.get_dummies(data[0], prefix=\"parents\")\n",
    "    has_nurs = pd.get_dummies(data[1], prefix=\"has_nurs\")\n",
    "    form = pd.get_dummies(data[2], prefix=\"form\")\n",
    "    children = pd.get_dummies(data[3], prefix=\"children\")\n",
    "    housing = pd.get_dummies(data[4], prefix=\"housing\")\n",
    "    finance = pd.get_dummies(data[5], prefix=\"finance\")\n",
    "    social = pd.get_dummies(data[6], prefix=\"social\")\n",
    "    health = pd.get_dummies(data[7], prefix=\"health\")\n",
    "    train = pd.concat([parents, has_nurs, form, children, housing,\n",
    "                    finance, social, health], axis=1)\n",
    "    cols = ['parents_usual', 'parents_pretentious', 'parents_great_pret', 'has_nurs_proper', 'has_nurs_less_proper', 'has_nurs_improper',\n",
    "            'has_nurs_critical', 'has_nurs_very_crit',\n",
    "            'form_complete', 'form_completed', 'form_incomplete', 'form_foster',\n",
    "            'children_1', 'children_2', 'children_3', 'children_more',\n",
    "            'housing_convenient', 'housing_less_conv', 'housing_critical',\n",
    "            'finance_convenient', 'finance_inconv',\n",
    "            'social_nonprob', 'social_slightly_prob', 'social_problematic',\n",
    "            'health_recommended', 'health_priority', 'health_not_recom']\n",
    "    return train[pd.Index(cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 28) (5400, 5) (600, 28)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(sys.argv[1], header=None)\n",
    "test_data = pd.read_csv(sys.argv[2], header=None)\n",
    "train = hotencode_train(train_data)\n",
    "test = hotencode_test(test_data)\n",
    "classes = train.iloc[:, -5:].values\n",
    "features = train.iloc[:, :-5].values\n",
    "features = np.c_[np.ones(len(train)), features]\n",
    "X_train = features[:5400, :]\n",
    "Y_train = classes[:5400, :]\n",
    "X_test = features[5400:, :]\n",
    "Y_test = classes[5400:, :]\n",
    "print(X_train.shape, Y_train.shape,X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,W):\n",
    "    XW = np.exp(np.matmul(X, W))\n",
    "    denom = np.sum(XW, axis=1)\n",
    "    Y_predict = np.divide(XW, denom.reshape(X.shape[0], 1))\n",
    "    return np.matmul(X.transpose(), Y - Y_predict)/X.shape[0]\n",
    "def cost(W,X,Y):\n",
    "    XW = np.exp(np.matmul(X,W))\n",
    "    logTerm = np.log(np.sum(XW,axis=1))\n",
    "    weightedTerm = np.sum(np.multiply(np.matmul(Y,W.T),X),axis=1)\n",
    "    error = np.sum(logTerm-weightedTerm)/X.shape[0]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21645523518424617 0.2164652335432906\n",
      "3486\n",
      "micro_F1_score=  0.9133333333333333 0 0.9133333333333333\n",
      "updated\n",
      "(38, 0.8702004139948987)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file:57: RuntimeWarning: invalid value encountered in double_scalars\n",
      "file:58: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21820424575790184 0.21821424189997174\n",
      "3444\n",
      "micro_F1_score=  0.9133333333333333 0 0.9133333333333333\n",
      "0.23594372376460762 0.23595371781976315\n",
      "3002\n",
      "micro_F1_score=  0.915 0 0.915\n",
      "updated\n",
      "0.3468926537423164 0.3469026420469391\n",
      "1350\n",
      "micro_F1_score=  0.9133333333333333 0 0.9133333333333333\n",
      "0.7007174728398741 0.7007273885027991\n",
      "369\n",
      "micro_F1_score=  0.9016666666666667 0 0.9016666666666666\n",
      "1.2827472247041707 1.282756842579927\n",
      "67\n",
      "micro_F1_score=  0.8666666666666667 0 0.8666666666666667\n",
      "1.5577928605920721 1.5577933854063788\n",
      "11\n",
      "micro_F1_score=  0.7983333333333333 0 0.7983333333333333\n",
      "(34, nan)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file:4: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999, nan)\n",
      "micro_F1_score=  0.31 0 0.31\n",
      "99999, nan)\n",
      "micro_F1_score=  0.31 0 0.31\n",
      "99999, nan)\n",
      "micro_F1_score=  0.31 0 0.31\n",
      "0.2037589439425205 0.2037689427238259\n",
      "2641\n",
      "micro_F1_score=  0.9183333333333333 1 0.9183333333333333\n",
      "updated\n",
      "0.2057895984029242 0.20579959460863867\n",
      "2600\n",
      "micro_F1_score=  0.9183333333333333 1 0.9183333333333333\n",
      "0.22672791960685337 0.22673791688788125\n",
      "2149\n",
      "micro_F1_score=  0.915 1 0.915\n",
      "0.34469179623188684 0.3447017833201655\n",
      "829\n",
      "micro_F1_score=  0.9133333333333333 1 0.9133333333333333\n",
      "0.7004151717024818 0.7004251209294677\n",
      "206\n",
      "micro_F1_score=  0.9016666666666667 1 0.9016666666666666\n",
      "1.2826947313220114 1.282704048154966\n",
      "37\n",
      "micro_F1_score=  0.8633333333333333 1 0.8633333333333333\n",
      "1.5577608047165503 1.557760804728818\n",
      "11\n",
      "micro_F1_score=  0.7783333333333333 1 0.7783333333333333\n",
      "99999, nan)\n",
      "micro_F1_score=  0.31 1 0.31\n",
      "99999, nan)\n",
      "micro_F1_score=  0.31 1 0.31\n",
      "(6940, nan)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-3e06af4585af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mgrad\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mw_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# w_initial = (w_initial.T - w_initial.T[0]).T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-7fb0c0209e26>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(X, Y, W)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mY_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mXW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = [0.00001,0.0001,0.001,0.01, .1, 1,10,2500,25000,30000]\n",
    "# alpha = [0.1,.02]\n",
    "batch = [10,20,30]\n",
    "lamIndex=0\n",
    "batchIndex=0\n",
    "a = np.zeros((len(alpha),len(batch)))\n",
    "f1= 0\n",
    "for m in range(0,len(batch)):\n",
    "    for n in range(0,len(alpha)):\n",
    "        X_train = np.copy(features[:5400, :])\n",
    "        Y_train = np.copy(classes[:5400, :])\n",
    "        X_test = np.copy(features[5400:, :])\n",
    "        Y_test = np.copy(classes[5400:, :])\n",
    "        w_initial = np.zeros(28*5).reshape(28, 5)\n",
    "        # w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "        costAr = []\n",
    "        k = batch[m]\n",
    "        l = X_train.shape[0]\n",
    "        costAr.append(0)\n",
    "        for j in range(1, 10000):\n",
    "            for i in range(0,k):\n",
    "                grad  = gradient(X_train[int((l/k)*i):int((l/k)*(i+1)),:] , Y_train[int((l/k)*i):int((l/k)*(i+1)),:], w_initial)\n",
    "                w_initial = w_initial+0.01*(grad-alpha[n]*w_initial)\n",
    "                # w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "            c = cost(w_initial,X_train,Y_train)\n",
    "            print((j,c), end=\"\\r\", flush=True)\n",
    "            costAr.append(c)\n",
    "            if(costAr[j-1]-costAr[j]<.00001 and j>10):\n",
    "                print(costAr[j],costAr[j-1])\n",
    "                break\n",
    "        print(j)\n",
    "        w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "        WmulX = np.exp(np.matmul(X_test, w_initial))\n",
    "        denom = np.sum(WmulX, axis=1)\n",
    "        Y_predict = np.divide(WmulX, denom.reshape(len(X_test), 1))\n",
    "        b = np.zeros_like(Y_predict)\n",
    "        b[np.arange(len(Y_predict)), Y_predict.argmax(1)] = 1\n",
    "        Y_predict = b\n",
    "        accuracy = np.trace(np.matmul(Y_predict, Y_test.T))/len(X_test)\n",
    "        Y_predict[:,0] =1*Y_predict[:,0]\n",
    "        Y_predict[:,1] =2*Y_predict[:,1]\n",
    "        Y_predict[:,2] =3*Y_predict[:,2]\n",
    "        Y_predict[:,3] =4*Y_predict[:,3]\n",
    "        Y_predict[:,4] =5*Y_predict[:,4]\n",
    "        Y_predict = np.sum(Y_predict,axis=1).tolist()\n",
    "        Y_test[:,0] =1*Y_test[:,0]\n",
    "        Y_test[:,1] =2*Y_test[:,1]\n",
    "        Y_test[:,2] =3*Y_test[:,2]\n",
    "        Y_test[:,3] =4*Y_test[:,3]\n",
    "        Y_test[:,4] =5*Y_test[:,4]\n",
    "        Y_test = np.sum(Y_test,axis=1).tolist()\n",
    "        mat = np.zeros((5,5))\n",
    "        f1Score = []\n",
    "        for i in range(0,len(Y_predict)):\n",
    "            mat[int(Y_predict[i])-1][Y_test[i]-1]=mat[int(Y_predict[i])-1][Y_test[i]-1]+1\n",
    "        for i in range(len(mat)):\n",
    "            precision = mat[i,i]/np.sum(mat[:,i])\n",
    "            recall = mat[i,i]/np.sum(mat[i,:])\n",
    "            f1Score.append([mat[i,i],np.sum(mat[:,i])-mat[i,i],np.sum(mat[i,:])-mat[i,i],precision,recall,precision*recall/(precision+recall)*100])  \n",
    "        F1Score = pd.DataFrame([],columns=[\"TP\",\"FP\",\"FN\",\"precision\",\"recall\",\"F1_Score\"])\n",
    "        F1Score.loc[\"class_not_recom\"] = f1Score[0]\n",
    "        F1Score.loc[\"class_recommend\"] = f1Score[1]\n",
    "        F1Score.loc[\"class_very_recom\"] = f1Score[2]\n",
    "        F1Score.loc[\"class_priority\"] = f1Score[3]\n",
    "        F1Score.loc[\"class_spec_prior\"] = f1Score[4]\n",
    "        micro_average_precision = np.sum(F1Score.iloc[:,0].values)/(np.sum(F1Score.iloc[:,0].values)+np.sum(F1Score.iloc[:,1].values))\n",
    "        micro_average_recall = np.sum(F1Score.iloc[:,0].values)/(np.sum(F1Score.iloc[:,0].values)+np.sum(F1Score.iloc[:,2].values))\n",
    "        micro_F1_score = micro_average_precision*micro_average_recall/(micro_average_precision+micro_average_recall)\n",
    "        print(\"micro_F1_score= \",2*micro_F1_score,m,accuracy)\n",
    "        if(f1<2*micro_F1_score and micro_F1_score==micro_F1_score):\n",
    "            print(\"updated\")\n",
    "            f1= 2*micro_F1_score\n",
    "            lamIndex=n\n",
    "            batchIndex = m\n",
    "        a[n][m]=2*micro_F1_score\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.000005,0.00001,0.00005,0.0001, .001, .01,0.1,1,10]\n",
    "batch = [10,1]\n",
    "a = np.zeros((len(alpha),len(batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7059994031552381 0.7060093910528225\n",
      "2314\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.1,.02]\n",
    "batch = [1]\n",
    "lamIndex=0\n",
    "batchIndex=0\n",
    "a = np.zeros((len(alpha),len(batch)))\n",
    "f1= 0\n",
    "# for m in range(0,len(batch)):\n",
    "# for n in range(0,len(alpha)):\n",
    "X_train = np.copy(features[:5400, :])\n",
    "Y_train = np.copy(classes[:5400, :])\n",
    "X_test = np.copy(features[5400:, :])\n",
    "Y_test = np.copy(classes[5400:, :])\n",
    "w_initial = np.zeros(28*5).reshape(28, 5)\n",
    "# w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "costAr = []\n",
    "k = batch[0]\n",
    "l = X_train.shape[0]\n",
    "costAr.append(0)\n",
    "for j in range(1, 10000):\n",
    "    for i in range(0,k):\n",
    "        grad  = gradient(X_train[int((l/k)*i):int((l/k)*(i+1)),:] , Y_train[int((l/k)*i):int((l/k)*(i+1)),:], w_initial)\n",
    "        w_initial = w_initial+0.01*(grad-alpha[0]*w_initial)\n",
    "        # w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "    c = cost(w_initial,X_train,Y_train)\n",
    "    print((j,c), end=\"\\r\", flush=True)\n",
    "    costAr.append(c)\n",
    "    if(costAr[j-1]-costAr[j]<.00001 and j>10):\n",
    "        print(costAr[j],costAr[j-1])\n",
    "        break\n",
    "print(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 4, 0],\n",
       "       [0, 0, 0, 4, 0],\n",
       "       [0, 0, 0, 0, 5]], dtype=uint8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.copy(features[:5400, :])\n",
    "Y_train = np.copy(classes[:5400, :])\n",
    "X_test = np.copy(features[5400:, :])\n",
    "Y_test = np.copy(classes[5400:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = np.sum(Y_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "WmulX = np.exp(np.matmul(X_test, w_initial))\n",
    "denom = np.sum(WmulX, axis=1)\n",
    "Y_predict = np.divide(WmulX, denom.reshape(len(X_test), 1))\n",
    "b = np.zeros_like(Y_predict)\n",
    "b[np.arange(len(Y_predict)), Y_predict.argmax(1)] = 1\n",
    "Y_predict = b\n",
    "accuracy = np.trace(np.matmul(Y_predict, Y_test.T))/len(X_test)\n",
    "Y_predict[:,0] =1*Y_predict[:,0]\n",
    "Y_predict[:,1] =2*Y_predict[:,1]\n",
    "Y_predict[:,2] =3*Y_predict[:,2]\n",
    "Y_predict[:,3] =4*Y_predict[:,3]\n",
    "Y_predict[:,4] =5*Y_predict[:,4]\n",
    "Y_predict = np.sum(Y_predict,axis=1).tolist()\n",
    "Y_test[:,0] =1*Y_test[:,0]\n",
    "Y_test[:,1] =2*Y_test[:,1]\n",
    "Y_test[:,2] =3*Y_test[:,2]\n",
    "Y_test[:,3] =4*Y_test[:,3]\n",
    "Y_test[:,4] =5*Y_test[:,4]\n",
    "Y_test = np.sum(Y_test,axis=1).tolist()\n",
    "mat = np.zeros((5,5))\n",
    "#     if(f1<micro_F1_score):\n",
    "#         print(\"updated\")\n",
    "#         lamIndex=n\n",
    "#         batchIndex = 0\n",
    "#     a[0][m]=micro_F1_score\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[186.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,  15., 200.,  29.],\n",
       "       [  0.,   0.,   0.,  15., 155.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1Score = []\n",
    "for i in range(0,len(Y_predict)):\n",
    "    mat[int(Y_predict[i])-1][Y_test[i]-1]=mat[int(Y_predict[i])-1][Y_test[i]-1]+1\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(mat)):\n",
    "    TP = mat[i,i]\n",
    "    FP = np.sum(mat[:,i])-mat[i,i]\n",
    "    FN = np.sum(mat[i,:])-mat[i,i]\n",
    "    if(TP==0 and FP==0 and FN==0):\n",
    "        precision=100\n",
    "        recall=100\n",
    "        F1_score=100\n",
    "    elif(TP==0 and (FP==0 or FN==0)):\n",
    "        precision=0\n",
    "        recall=0\n",
    "        F1_score=0\n",
    "    else:\n",
    "        precision = mat[i,i]/np.sum(mat[:,i])\n",
    "        recall = mat[i,i]/np.sum(mat[i,:])\n",
    "        F1_score = 2*precision*recall/(precision+recall)*100\n",
    "    f1Score.append([TP,FP,FN,precision,recall,F1_score])  \n",
    "\n",
    "# micro_average_precision = np.sum(F1Score.iloc[:,0].values)/(np.sum(F1Score.iloc[:,0].values)+np.sum(F1Score.iloc[:,1].values))\n",
    "# micro_average_recall = np.sum(F1Score.iloc[:,0].values)/(np.sum(F1Score.iloc[:,0].values)+np.sum(F1Score.iloc[:,2].values))\n",
    "# micro_F1_score = micro_average_precision*micro_average_recall/(micro_average_precision+micro_average_recall)\n",
    "# print(\"micro_F1_score= \",2*micro_F1_score,accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_F1_score=  0.9016666666666667 0.9016666666666666\n"
     ]
    }
   ],
   "source": [
    "micro_average_precision = np.sum(F1Score.iloc[:,0].values)/(np.sum(F1Score.iloc[:,0].values)+np.sum(F1Score.iloc[:,1].values))\n",
    "micro_average_recall = np.sum(F1Score.iloc[:,0].values)/(np.sum(F1Score.iloc[:,0].values)+np.sum(F1Score.iloc[:,2].values))\n",
    "micro_F1_score = micro_average_precision*micro_average_recall/(micro_average_precision+micro_average_recall)\n",
    "print(\"micro_F1_score= \",2*micro_F1_score,accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_not_recom</th>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_recommend</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_very_recom</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_priority</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>87.145969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_spec_prior</th>\n",
       "      <td>155.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.842391</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>87.570621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TP    FP    FN   precision      recall    F1_Score\n",
       "class_not_recom   186.0   0.0   0.0    1.000000    1.000000  100.000000\n",
       "class_recommend     0.0   0.0   0.0  100.000000  100.000000  100.000000\n",
       "class_very_recom    0.0  15.0   0.0    0.000000    0.000000    0.000000\n",
       "class_priority    200.0  15.0  44.0    0.930233    0.819672   87.145969\n",
       "class_spec_prior  155.0  29.0  15.0    0.842391    0.911765   87.570621"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1Score = pd.DataFrame([],columns=[\"TP\",\"FP\",\"FN\",\"precision\",\"recall\",\"F1_Score\"])\n",
    "F1Score.loc[\"class_not_recom\"] = f1Score[0]\n",
    "F1Score.loc[\"class_recommend\"] = f1Score[1]\n",
    "F1Score.loc[\"class_very_recom\"] = f1Score[2]\n",
    "F1Score.loc[\"class_priority\"] = f1Score[3]\n",
    "F1Score.loc[\"class_spec_prior\"] = f1Score[4]\n",
    "Weighted_F1_score = np.dot(cl,F1Score.iloc[:,5].values)/np.sum(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.08229632091381"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weighted_F1_score = np.dot(cl,F1Score.iloc[:,5].values)/np.sum(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[186.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,  15., 200.,  29.],\n",
       "       [  0.,   0.,   0.,  15., 155.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.297173260700423270042327) 0.2971832603049492\n",
      "0.9133333333333333\n",
      "0.2972144106680319 0.29722441005367906\n",
      "0.9133333333333333\n",
      "0.2975536255440337 0.2975636252465655\n",
      "0.9133333333333333\n",
      "0.2979751828725619 0.2979851824334659\n",
      "0.9133333333333333\n",
      "0.30557438741251436 0.3055843852475009\n",
      "0.915\n",
      "0.374876909888105 0.3748869094203459\n",
      "0.9116666666666666\n",
      "0.7059994031552381 0.7060093910528225\n",
      "0.9016666666666666\n",
      "1.2835392340571263 1.2835492181785806\n",
      "0.8666666666666667\n",
      "1.5578892523086028 1.5578987171693646\n",
      "0.81\n",
      "[0.91333333 0.91333333 0.91333333 0.91333333 0.915      0.91166667\n",
      " 0.90166667 0.86666667 0.81      ]\n"
     ]
    }
   ],
   "source": [
    "costOverall = []\n",
    "a=np.zeros(len(alpha))\n",
    "for n in range(0,len(alpha)):\n",
    "    w_initial = np.ones(28*5).reshape(28, 5)\n",
    "    w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "    costAr = []\n",
    "    lr = .01\n",
    "    costAr.append(0)\n",
    "    for j in range(1, 10000):\n",
    "        grad = gradient(X_train,Y_train,w_initial)\n",
    "        w_initial = w_initial+lr*(grad-alpha[n]*w_initial)\n",
    "        c = cost(w_initial,X_train,Y_train)\n",
    "        print((j,c), end=\"\\r\", flush=True)\n",
    "        costAr.append(c)\n",
    "        if(costAr[j-1]-costAr[j]<.00001 and j>10):\n",
    "            print(costAr[j],costAr[j-1])\n",
    "            break\n",
    "    WmulX = np.exp(np.matmul(X_test, w_initial))\n",
    "    denom = np.sum(WmulX, axis=1)\n",
    "    Y_predict = np.divide(WmulX, denom.reshape(len(X_test), 1))\n",
    "    b = np.zeros_like(Y_predict)\n",
    "    b[np.arange(len(Y_predict)), Y_predict.argmax(1)] = 1\n",
    "    Y_predict = b\n",
    "    accuracy = np.trace(np.matmul(Y_predict, Y_test.T))/len(X_test)\n",
    "    print(accuracy)\n",
    "    a[n]=accuracy        \n",
    "    costOverall.append(costAr)  \n",
    "print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(0,len(batch)):\n",
    "    for n in range(0,len(alpha)):\n",
    "        w_initial = np.zeros(28*5).reshape(28, 5)\n",
    "        # w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "        costAr = []\n",
    "        k = batch[m]\n",
    "        l = X_train.shape[0]\n",
    "        costAr.append(0)\n",
    "        for j in range(1, 10000):\n",
    "            for i in range(0,k):\n",
    "                grad  = gradient(X_train[int((l/k)*i):int((l/k)*(i+1)),:] , Y_train[int((l/k)*i):int((l/k)*(i+1)),:], w_initial)\n",
    "                w_initial = w_initial+0.01*(grad-alpha[n]*w_initial)\n",
    "                # w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "            c = cost(w_initial,X_train,Y_train)\n",
    "            print((j,c), end=\"\\r\", flush=True)\n",
    "            costAr.append(c)\n",
    "            if(costAr[j-1]-costAr[j]<.00001 and j>10):\n",
    "                print(costAr[j],costAr[j-1])\n",
    "                break\n",
    "        print(j)\n",
    "        w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "        WmulX = np.exp(np.matmul(X_test, w_initial))\n",
    "        denom = np.sum(WmulX, axis=1)\n",
    "        Y_predict = np.divide(WmulX, denom.reshape(len(X_test), 1))\n",
    "        b = np.zeros_like(Y_predict)\n",
    "        b[np.arange(len(Y_predict)), Y_predict.argmax(1)] = 1\n",
    "        Y_predict = b\n",
    "        accuracy = np.trace(np.matmul(Y_predict, Y_test.T))/len(X_test)\n",
    "        print(m,accuracy)\n",
    "        a[n][m]=accuracy\n",
    "print(a)\n",
    "X_train = features[:, :]\n",
    "X_train[:,21]=0\n",
    "Y_train = classes[:, :]\n",
    "print(X_train.shape,Y_train.shape)\n",
    "lamIndex, batchIndex =  np.where(a==np.max(a))[0][0], np.where(a==np.max(a))[1][0]\n",
    "w_initial = np.ones(28*5).reshape(28, 5)\n",
    "w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "costAr = []\n",
    "k = batch[batchIndex]\n",
    "l = X_train.shape[0]\n",
    "costAr.append(0)\n",
    "for j in range(1, 10000):\n",
    "    for i in range(0,k):\n",
    "        grad  = gradient(X_train[int((l/k)*i):int((l/k)*(i+1)),:] , Y_train[int((l/k)*i):int((l/k)*(i+1)),:], w_initial)\n",
    "        w_initial = w_initial+0.01*(grad-alpha[lamIndex]*w_initial)\n",
    "        w_initial = (w_initial.T - w_initial.T[0]).T\n",
    "    c = cost(w_initial,X_train,Y_train)\n",
    "    print((j,c), end=\"\\r\", flush=True)\n",
    "    costAr.append(c)\n",
    "    if(costAr[j-1]-costAr[j]<.00001 and j>10):\n",
    "        print(costAr[j],costAr[j-1])\n",
    "        break\n",
    "print(j)\n",
    "features = test.iloc[:, :].values\n",
    "features = np.c_[np.ones(len(test)), features]\n",
    "X_test = features[:, :]\n",
    "X_test[:,21]=0\n",
    "WmulX = np.exp(np.matmul(X_test, w_initial))\n",
    "denom = np.sum(WmulX, axis=1)\n",
    "Y_predict = np.divide(WmulX, denom.reshape(len(X_test), 1))\n",
    "b = np.zeros_like(Y_predict)\n",
    "b[np.arange(len(Y_predict)), Y_predict.argmax(1)] = 1\n",
    "Y_predict = b\n",
    "Y_predict[:, 0] = 1*Y_predict[:, 0]\n",
    "Y_predict[:, 1] = 2*Y_predict[:, 1]\n",
    "Y_predict[:, 2] = 3*Y_predict[:, 2]\n",
    "Y_predict[:, 3] = 4*Y_predict[:, 3]\n",
    "Y_predict[:, 4] = 5*Y_predict[:, 4]\n",
    "Y_predict = np.sum(Y_predict, axis=1).tolist()\n",
    "predict = []\n",
    "for i in range(0, len(Y_predict)):\n",
    "    if(Y_predict[i] == 1):\n",
    "        predict.append(\"not_recom\")\n",
    "    elif(Y_predict[i] == 2):\n",
    "        predict.append(\"recommend\")\n",
    "    elif(Y_predict[i] == 3):\n",
    "        predict.append(\"very_recom\")\n",
    "    elif(Y_predict[i] == 4):\n",
    "        predict.append(\"priority\")\n",
    "    elif(Y_predict[i] == 5):\n",
    "        predict.append(\"spec_prior\")\n",
    "Y_predict = pd.DataFrame(predict)\n",
    "Y_predict.to_csv(sys.argv[3], header=False, index=False)\n",
    "w_initial = pd.DataFrame(w_initial)\n",
    "w_initial.to_csv(sys.argv[4], header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
